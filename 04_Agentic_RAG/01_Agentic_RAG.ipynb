{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5be79d9",
   "metadata": {},
   "source": [
    "### Agentic RAG\n",
    "#### RAG (Retrieval-Augmented Generation):\n",
    "- RAG is a technique where a retriever fetches relevant information from external knowledge sources (like a vector database, documents, APIs), and then a generator (LLM) uses this context to produce accurate and up-to-date answers.  \n",
    "\n",
    "ðŸ‘‰ Limitation: Traditional RAG is static. It retrieves once, feeds the result to the LLM, and generates an answer. If retrieval fails (wrong doc, incomplete info), the model may hallucinate or produce poor answers.  \n",
    "\n",
    "#### Agentic RAG (Agent + RAG):\n",
    "Agentic RAG enhances this by turning the LLM into an autonomous agent that can:\n",
    "- Reason about whether retrieved information is sufficient.\n",
    "- Decide actions dynamically (e.g., re-query, filter, call tools, ask clarifying questions).\n",
    "- Iterate until it gathers the right information before generating the final answer.\n",
    "- Use multiple tools (search, APIs, calculators, SQL queries, etc.) in addition to retrieval.\n",
    "Essentially, Agentic RAG = RAG + Agency (decision-making loop).  \n",
    "\n",
    "#### Example Flow (Traditional vs Agentic RAG):\n",
    "ðŸ”¹ **Traditional RAG**\n",
    "- User: â€œWhat are the latest treatments for Type 2 Diabetes?â€\n",
    "- System: Retrieves 3 documents â†’ feeds to LLM â†’ generates answer (may be outdated if docs are stale).  \n",
    "\n",
    "ðŸ”¹ **Agentic RAG**\n",
    "- LLM acts as an agent:\n",
    "    1. Retrieves docs.\n",
    "    2. Realizes results are from 2022 only.\n",
    "    3. Decides to re-query with â€œType 2 Diabetes treatment 2023 2024 site:pubmed.govâ€.\n",
    "    4. Fetches newer research papers.\n",
    "    5. Summarizes & cites.\n",
    "- User gets a reliable, updated answer.\n",
    "    \n",
    "- **Summary**: Agentic RAG is Retrieval-Augmented Generation combined with an agentic loop, where the LLM doesnâ€™t just consume retrieved context passively but actively decides how to retrieve, re-query, call external tools, and iterate until it gathers the right knowledge before responding. It reduces hallucination and makes AI systems more accurate, adaptive, and reliable for enterprise use cases.\n",
    "\n",
    "#### Why It Matters\n",
    "- Improves accuracy (less hallucination).\n",
    "- Enables multi-step reasoning.\n",
    "- Can handle dynamic knowledge sources (databases, APIs).\n",
    "- Used in production-grade AI apps like copilots, research assistants, BFSI/healthcare chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7dee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Machine learning is a method of data analysis that involves building algorithms and statistical models that enable a computer system to learn and improve from experience without being explicitly programmed. It is a type of artificial intelligence that uses patterns and inference to make decisions and predictions based on data. Machine learning involves training algorithms to recognize patterns and make decisions based on that data, with the goal of improving the system's performance over time.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 12, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CbV4themnXjYbIpF9ljEND2bG44Xy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a88b2149-2d46-4c06-8fdb-7dfeb6ecea8f-0', usage_metadata={'input_tokens': 12, 'output_tokens': 80, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from typing import Annotated, List, Sequence\n",
    "import operator\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Load LLM\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(name=\"gpt-5-nano\", api_key=api_key)\n",
    "llm.invoke(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6b6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG imports\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "284c87f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Graph API overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimecloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry pointConditional entry pointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph migrationsRuntime contextRecursion limitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n\\nState: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.\\n\\n\\nBy composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state.\\nTo emphasize: Nodes and Edges are nothing more than functions â€“ they can contain an LLM or just good olâ€™ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraphâ€™s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Googleâ€™s Pregel system, the program proceeds in discrete â€œsuper-steps.â€\\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or â€œchannelsâ€). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?\\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)\\n\\nYou MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that Pydantic is less performant than a TypedDict or dataclass).\\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide for more information.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\\n\\nInternal nodes can pass information that is not required in the graphâ€™s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an â€œinternalâ€ schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the â€œinternalâ€ schema to constrain the input and output of the graph. See this guide for more detail.\\nLetâ€™s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.\\n\\n\\nWe initialize the graph with:\\nCopyAsk AIStateGraph(\\n    OverallState,\\n    input_schema=InputState,\\n    output_schema=OutputState\\n)\\n\\nSo, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization?\\nWe can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample ACopyAsk AIfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Letâ€™s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Letâ€™s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample BCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n\\nIn this example, weâ€™ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Letâ€™s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Letâ€™s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChainâ€™s chat model interface in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response).\\nTo read more about what message objects are, please refer to the Messages conceptual guide.\\n\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you donâ€™t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.\\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel.\\nSee more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}\\n\\n# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content.\\nBelow is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate â€“ The state of the graph\\nconfig â€“ A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime â€“ A Runtime object that contains runtime context and other information like store and stream_writer\\n\\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...\\n\\nBehind the scenes, functions are converted to RunnableLambda, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}\\n\\n\\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\\n\\nNormal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have multiple outgoing edges. If a node has multiple outgoing edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a â€œrouting functionâ€ to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)\\n\\nSimilar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_functionâ€™s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional entry point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_functionâ€™s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\n\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]\\n\\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\\n\\nWhen returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where itâ€™s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.\\n\\n\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nSetting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key thatâ€™s shared by both parent and subgraph state schemas, you must define a reducer for the key youâ€™re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\\n\\nFor threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) â€” if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change â€” if this is a blocker please reach out and we can prioritize a solution.\\n\\n\\u200bRuntime context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...\\n\\nSee this guide for a full breakdown on configuration.\\n\\u200bRecursion limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\\n\\nRead this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\n\\ndef my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }\\n\\n    # Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }\\n\\n    # Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nItâ€™s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Functional API overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimecloseOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraphâ€™s key features â€” persistence, memory, human-in-the-loop, and streaming â€” to your applications with minimal changes to your existing code.\\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:\\n\\n@entrypoint â€“ Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task â€“ Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraphâ€™s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:\\n\\nControl flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.\\nVisualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.\\n\\n\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"\\n\\n@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }\\n\\nDetailed ExplanationThis workflow will write an essay about the topic â€œcatâ€ and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"\\n\\n@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }\\n\\n\\nthread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True\\n\\nfor item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{\\'workflow\\': {\\'essay\\': \\'An essay about topic: cat\\', \\'is_approved\\': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\\nYou will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result\\n\\nSerialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore\\n\\nin_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously\\n\\n\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\\n\\n\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result\\n\\nSerialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you donâ€™t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If youâ€™re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.\\n\\n\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.\\n\\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.\\nLangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value\\n\\n\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) â†’ interrupt â†’ resume â†’ (returns 5 again) â†’ â€¦\\nNot in a task: Get random number (5) â†’ interrupt â†’ resume â†’ get new random number (7) â†’ â€¦\\n\\nThis is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, itâ€™s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\"https://docs.langchain.com/oss/python/langgraph/graph-api\",\n",
    "        \"https://docs.langchain.com/oss/python/langgraph/functional-api\"]\n",
    "\n",
    "docs = WebBaseLoader(urls).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5922680",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645c33a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='53a347b0-32cf-4c13-8d55-8386af2d0b35', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or â€œchannelsâ€). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?'),\n",
       " Document(id='607de276-f0a2-45bc-82d7-63b97876ab2e', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGraph migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.'),\n",
       " Document(id='b5f6f3c5-eff0-414c-87a5-c14d55ecfc08', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate â€“ The state of the graph\\nconfig â€“ A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime â€“ A Runtime object that contains runtime context and other information like store and stream_writer'),\n",
       " Document(id='939c9616-2fc6-4f79-86b2-82b3405bd1d2', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Graph API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimecloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is a StateGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1dc5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='GraphAPI', description='Search and run information about graph api', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000144A746CA60>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000144A7808580>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000144A746C9D0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000144A7808580>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert retriever to retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "graph_api_tool = create_retriever_tool(retriever=retriever,\n",
    "                      name=\"GraphAPI\",\n",
    "                      description=\"Search and run information about graph api\")\n",
    "graph_api_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49b21c",
   "metadata": {},
   "source": [
    "### Create another vector store\n",
    "The idea is to have 2 tools (for demo), each tool having separate vectorstore and we let LLM decide which one to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f5f498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilitycloseOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\\ncreate_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\\n\\ncreate_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyAsk AIfrom langchain.agents import create_agent\\n\\nagent = create_agent(\\n    \"gpt-5\",\\n    tools=tools\\n)\\n\\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)\\n\\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\\n\\u200bDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\\nCopyAsk AIfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\n\\n\\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\\n\\n@wrap_model_call\\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\\n    \"\"\"Choose model based on conversation complexity.\"\"\"\\n    message_count = len(request.state[\"messages\"])\\n\\n    if message_count > 10:\\n        # Use an advanced model for longer conversations\\n        model = advanced_model\\n    else:\\n        model = basic_model\\n\\n    request.model = model\\n    return handler(request)\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)\\n\\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\\n\\u200bTools\\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\\n\\nMultiple tool calls in sequence (triggered by a single prompt)\\nParallel tool calls when appropriate\\nDynamic tool selection based on previous results\\nTool retry logic and error handling\\nState persistence across tool calls\\n\\nFor more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nCopyAsk AIfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72Â°F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])\\n\\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\\n\\u200bTool error handling\\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_tool_call\\nfrom langchain_core.messages import ToolMessage\\n\\n\\n@wrap_tool_call\\ndef handle_tool_errors(request, handler):\\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\\n    try:\\n        return handler(request)\\n    except Exception as e:\\n        # Return a custom error message to the model\\n        return ToolMessage(\\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\\n            tool_call_id=request.tool_call[\"id\"]\\n        )\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)\\n\\nThe agent will return a ToolMessage with the custom error message when a tool fails:\\nCopyAsk AI[\\n    ...\\n    ToolMessage(\\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\\n        tool_call_id=\"...\"\\n    ),\\n    ...\\n]\\n\\n\\u200bTool use in the ReAct loop\\nAgents follow the ReAct (â€œReasoning + Actingâ€) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.CopyAsk AI================================ Human Message =================================\\n\\nFind the most popular wireless headphones right now and check if they\\'re in stock\\n\\nReasoning: â€œPopularity is time-sensitive, I need to use the provided search tool.â€\\nActing: Call search_products(\"wireless headphones\")\\nCopyAsk AI================================== Ai Message ==================================\\nTool Calls:\\n  search_products (call_abc123)\\n Call ID: call_abc123\\n  Args:\\n    query: wireless headphones\\nCopyAsk AI================================= Tool Message =================================\\n\\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\\n\\nReasoning: â€œI need to confirm availability for the top-ranked item before answering.â€\\nActing: Call check_inventory(\"WH-1000XM5\")\\nCopyAsk AI================================== Ai Message ==================================\\nTool Calls:\\n  check_inventory (call_def456)\\n Call ID: call_def456\\n  Args:\\n    product_id: WH-1000XM5\\nCopyAsk AI================================= Tool Message =================================\\n\\nProduct WH-1000XM5: 10 units in stock\\n\\nReasoning: â€œI have the most popular model and its stock status. I can now answer the userâ€™s question.â€\\nActing: Produce final answer\\nCopyAsk AI================================== Ai Message ==================================\\n\\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\\n\\nTo learn more about tools, see Tools.\\n\\u200bSystem prompt\\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\\nCopyAsk AIagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\\n)\\n\\nWhen no system_prompt is provided, the agent will infer its task from the messages directly.\\n\\u200bDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\\nThe @dynamic_prompt decorator creates middleware that generates system prompts dynamically based on the model request:\\nCopyAsk AIfrom typing import TypedDict\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass Context(TypedDict):\\n    user_role: str\\n\\n@dynamic_prompt\\ndef user_role_prompt(request: ModelRequest) -> str:\\n    \"\"\"Generate system prompt based on user role.\"\"\"\\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\\n    base_prompt = \"You are a helpful assistant.\"\\n\\n    if user_role == \"expert\":\\n        return f\"{base_prompt} Provide detailed technical responses.\"\\n    elif user_role == \"beginner\":\\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\\n\\n    return base_prompt\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[web_search],\\n    middleware=[user_role_prompt],\\n    context_schema=Context\\n)\\n\\n# The system prompt will be set dynamically based on context\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\\n    context={\"user_role\": \"expert\"}\\n)\\n\\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\\n\\u200bInvocation\\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\\nCopyAsk AIresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in San Francisco?\"}]}\\n)\\n\\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\\nCopyAsk AIfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'john@example.com\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model providerâ€™s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\\nCopyAsk AIfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent\\n\\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent.\\n\\u200bDefining state via middleware\\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\\nCopyAsk AIfrom langchain.agents import AgentState\\nfrom langchain.agents.middleware import AgentMiddleware\\nfrom typing import Any\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nclass CustomMiddleware(AgentMiddleware):\\n    state_schema = CustomState\\n    tools = [tool1, tool2]\\n\\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\\n        ...\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\\nCopyAsk AIfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\nAs of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWeâ€™ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyAsk AIfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")\\n\\nFor more details on streaming, see Streaming.\\n\\u200bMiddleware\\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\\n\\nProcess state before the model is called (e.g., message trimming, context injection)\\nModify or validate the modelâ€™s response (e.g., guardrails, content filtering)\\nHandle tool execution errors with custom logic\\nImplement dynamic model selection based on state or context\\nAdd custom logging, monitoring, or analytics\\n\\nMiddleware integrates seamlessly into the agentâ€™s execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.\\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = [\"https://docs.langchain.com/oss/python/langchain/agents\"]\n",
    "docs = WebBaseLoader(url).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd82b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever_langchain = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5afaaac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='79b2bd5e-2dd9-4b74-8a1f-36a72026f58e', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.'),\n",
       " Document(id='35b23a01-7328-4189-a128-13015f323a45', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='result = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'john@example.com\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model providerâ€™s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\\nCopyAsk AIfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)'),\n",
       " Document(id='f1df0122-4b98-4301-b2f4-e38bf9f4f6dc', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='As of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWeâ€™ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyAsk AIfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")'),\n",
       " Document(id='086d8e76-c3d1-4227-88e7-8b0c41b8223a', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='For streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\\nCopyAsk AIfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_langchain.invoke(\"How to return structured output?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835e81ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='LangChain', description='Search and run information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000144A746CA60>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000014504DC4AF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000144A746C9D0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000014504DC4AF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert retriever to retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "langchain_api_tool = create_retriever_tool(retriever=retriever_langchain,\n",
    "                      name=\"LangChain\",\n",
    "                      description=\"Search and run information about Langchain\")\n",
    "langchain_api_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be700520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [graph_api_tool, langchain_api_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471031c8",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b29e2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "106e1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state.\n",
    "    Given the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    state (messages) -- the current state\n",
    "    Return:\n",
    "    dict - The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    llm_with_tools = llm.bind_tools(tools=tools)\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "def grade_documents(state: AgentState) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines wheather the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    state (messages) -- the current state\n",
    "    Return:\n",
    "    str - A decision for wheather the documents are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    llm_with_tool = llm.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader, assessing the relevance of the retrieved documents to the user questions.\\n\n",
    "        Here is the retrieved document: \\n {context} \\n\\n\n",
    "        Here is the question: {question} \\n\n",
    "        If the documents contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    score_result = chain.invoke({\"context\": docs, \"question\": question})\n",
    "    score = score_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac390cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate(state: AgentState):\n",
    "    \"\"\"Generate answer\n",
    "    \n",
    "    Keyword arguments:\n",
    "    state (messages) -- the current state\n",
    "    Return:\n",
    "    dict - The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context}\n",
    "    Answer:\n",
    "    \"\"\")\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8a8b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state:AgentState):\n",
    "    \"\"\"Transform the query to produce a better question\n",
    "    \n",
    "    Keyword arguments:\n",
    "    state (messages) -- the current state\n",
    "    Return:\n",
    "    dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent/meaning. \\n\n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question}\n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question: \n",
    "        \"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1211471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define state graph and workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode(tools)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", tools_condition, {\n",
    "    \"tools\": \"retrieve\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "workflow.add_conditional_edges(\"retrieve\", grade_documents,\n",
    "{\n",
    "    \"generate\":\"generate\",\n",
    "    \"rewrite\":\"rewrite\"\n",
    "})\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View graph\n",
    "graph_image = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "display(Image(graph_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72224b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyDrive\\SelfStudyCode\\AI\\Krish\\AgenticAI\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1906: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are different ways to define State Schema?', additional_kwargs={}, response_metadata={}, id='37d18340-283b-4b1d-9063-345c0963f1b5'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_yemfVhMqRxWhrU4tELybjire', 'function': {'arguments': '{\"query\":\"State Schema\"}', 'name': 'GraphAPI'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 98, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CbdsW15HgP4h2j94TEL2dYqu0kjKd', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6f4a8b6b-1cd1-4cd1-be56-171d4dd5c109-0', tool_calls=[{'name': 'GraphAPI', 'args': {'query': 'State Schema'}, 'id': 'call_yemfVhMqRxWhrU4tELybjire', 'type': 'tool_call'}], usage_metadata={'input_tokens': 98, 'output_tokens': 15, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide for more information.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\\n\\nState: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.\\n\\nYou MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that Pydantic is less performant than a TypedDict or dataclass).\\n\\n\\u200bRuntime context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...', name='GraphAPI', id='9cf05abb-10be-4bd4-862f-3f0916d716a1', tool_call_id='call_yemfVhMqRxWhrU4tELybjire'),\n",
       "  HumanMessage(content='One way to define a State Schema is by using a TypedDict or a Pydantic model. Another approach is to specify reducer functions that dictate how updates are applied to the state. Additionally, you can define a context schema for runtime context passed to nodes in the graph.', additional_kwargs={}, response_metadata={}, id='59728601-c92e-4ea5-aefa-2f869d6f91b3')]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"What are different ways to define State Schema?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49b2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33c2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
