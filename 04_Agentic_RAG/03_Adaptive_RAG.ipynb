{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0e8ab3",
   "metadata": {},
   "source": [
    "### Adaptive RAG\n",
    "- Adaptive RAG is an enhanced Retrieval-Augmented Generation (RAG) technique where the system dynamically adjusts how it retrieves, selects, and uses knowledge based on the query, model confidence, and feedback signals.\n",
    "- It adapts its retrieval + generation strategy in real time to improve accuracy, reduce hallucination, and optimize performance.\n",
    "\n",
    "#### Simple Definition\n",
    "Adaptive RAG = RAG that adjusts retrieval depth, number of documents, retrieval methods, reasoning style, and even tool use based on the complexity of the user query or model uncertainty.\n",
    "\n",
    "#### Why Adaptive RAG?\n",
    "Traditional RAG always retrieves a fixed number of documents (e.g., top-3).  \n",
    "But:\n",
    "- Some questions need more documents\n",
    "- Some need none\n",
    "- Some need web search\n",
    "- Some need reasoning first, retrieval later\n",
    "- Some need reranking or cross-encoder verification\n",
    "Adaptive RAG automatically chooses what to do.\n",
    "\n",
    "#### Key Features of Adaptive RAG\n",
    "1. Dynamic Retrieval Depth\n",
    "The system decides:\n",
    "    - how many documents to retrieve,\n",
    "    - whether to retrieve at all,\n",
    "    - whether to re-retrieve.\n",
    "Example:\n",
    "    • If model uncertainty is high → increase retrieval (e.g., from 3 to 10 docs)\n",
    "    • If uncertainty is low → skip retrieval (faster, cheaper)\n",
    "\n",
    "2. Query Classification → Choose RAG strategy\n",
    "\n",
    "3. Adaptive Filtering & Reranking\n",
    "Uses:\n",
    "    - cross-encoder reranking\n",
    "    - vector + keyword hybrid search\n",
    "    - confidence-based filtering\n",
    "\n",
    "4. Multi-turn Retrieval (Iterative RAG)\n",
    "If answer is incomplete:\n",
    "    - Model says: “I need more info”\n",
    "    - System retrieves new documents\n",
    "    - LLM tries again\n",
    "This is similar to Corrective RAG (CRAG) but broader.\n",
    "\n",
    "5. Model Uncertainty Feedback Loop\n",
    "Model evaluates its own answer:\n",
    "    - If “confidence < threshold” → retrieve more documents\n",
    "    - If “confidence > threshold” → answer immediately\n",
    "\n",
    "6. Cost-Aware Adjustments\n",
    "Adaptive RAG reduces cost by:\n",
    "    - skipping retrieval when unnecessary\n",
    "    - using smaller models for retrieval\n",
    "    - escalating to larger LLM only for hard queries\n",
    "\n",
    "#### How to Implement Adaptive RAG (LangChain Example)\n",
    "Step 1: Classify Query  \n",
    "if is_simple_query(query):\n",
    "    top_k = 2\n",
    "elif is_expert_query(query):\n",
    "    top_k = 10\n",
    "else:\n",
    "    top_k = 5\n",
    "\n",
    "Step 2: Confidence-Based Retry  \n",
    "answer, confidence = llm_with_confidence(context, query)\n",
    "if confidence < 0.7:\n",
    "    context = retrieve_more_docs(query, top_k=10)\n",
    "    answer = llm(context, query)\n",
    "\n",
    "Step 3: Hybrid Retrieval  \n",
    "results = merge(\n",
    "    vector_store.search(query),\n",
    "    bm25.search(query)\n",
    ")\n",
    "\n",
    "#### Summary\n",
    "Adaptive RAG = RAG that adapts.\n",
    "It dynamically modifies retrieval strategies, answer generation, and reasoning pathways based on query type, model uncertainty, and context needs.\n",
    "This leads to:\n",
    "- fewer hallucinations\n",
    "- more accurate answers\n",
    "- lower cost\n",
    "- better performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c06ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_openai.chat_models.base.ChatOpenAI"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from typing import Annotated, List, Sequence\n",
    "import operator\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(name=\"gpt-5-nano\")\n",
    "llm.invoke(\"What is machine learning?\")\n",
    "type(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39599f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# RAG imports\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34b7da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Graph API overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry pointConditional entry pointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph migrationsRuntime contextRecursion limitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n\\nState: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.\\n\\n\\nBy composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state.\\nTo emphasize: Nodes and Edges are nothing more than functions – they can contain an LLM or just good ol’ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”\\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?\\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)\\n\\nYou MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that Pydantic is less performant than a TypedDict or dataclass).\\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide for more information.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\\n\\nInternal nodes can pass information that is not required in the graph’s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.\\nLet’s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.\\n\\n\\nWe initialize the graph with:\\nCopyAsk AIStateGraph(\\n    OverallState,\\n    input_schema=InputState,\\n    output_schema=OutputState\\n)\\n\\nSo, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization?\\nWe can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample ACopyAsk AIfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample BCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n\\nIn this example, we’ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let’s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s chat model interface in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response).\\nTo read more about what message objects are, please refer to the Messages conceptual guide.\\n\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.\\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel.\\nSee more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}\\n\\n# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content.\\nBelow is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate – The state of the graph\\nconfig – A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime – A Runtime object that contains runtime context and other information like store and stream_writer\\n\\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...\\n\\nBehind the scenes, functions are converted to RunnableLambda, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}\\n\\n\\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\\n\\nNormal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have multiple outgoing edges. If a node has multiple outgoing edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a “routing function” to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)\\n\\nSimilar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional entry point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\n\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]\\n\\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\\n\\nWhen returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.\\n\\n\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nSetting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\\n\\nFor threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.\\n\\n\\u200bRuntime context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...\\n\\nSee this guide for a full breakdown on configuration.\\n\\u200bRecursion limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\\n\\nRead this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\n\\ndef my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }\\n\\n    # Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }\\n\\n    # Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Functional API overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimeOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.\\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:\\n\\n@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:\\n\\nControl flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.\\nVisualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.\\n\\n\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"\\n\\n@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }\\n\\nDetailed ExplanationThis workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"\\n\\n@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }\\n\\n\\nthread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True\\n\\nfor item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{\\'workflow\\': {\\'essay\\': \\'An essay about topic: cat\\', \\'is_approved\\': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\\nYou will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result\\n\\nSerialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore\\n\\nin_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously\\n\\n\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\\n\\n\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result\\n\\nSerialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If you’re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.\\n\\n\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.\\n\\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.\\nLangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value\\n\\n\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) → interrupt → resume → (returns 5 again) → …\\nNot in a task: Get random number (5) → interrupt → resume → get new random number (7) → …\\n\\nThis is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\"https://docs.langchain.com/oss/python/langgraph/graph-api\",\n",
    "        \"https://docs.langchain.com/oss/python/langgraph/functional-api\"]\n",
    "\n",
    "docs = WebBaseLoader(urls).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26744bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdb4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyDrive\\SelfStudyCode\\AI\\Krish\\AgenticAI\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1906: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "# Data model - query analysis\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route the user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        description=\"Given a user query, choose to route it to web search or vectorstore\")\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert at routing the user question to a vectorstore or web search.\n",
    "The vectorstore contains the documents related to graph API and functional API of LangGraph.\n",
    "Use the vectorstore for questions on these topics. Otherwise use websearch\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"Who won the cricket world cup in 2023?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ede043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"How to create a stategraph?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa204b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyDrive\\SelfStudyCode\\AI\\Krish\\AgenticAI\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1906: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Data model - Retrieval grader\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are a grader assessing relevance of retrieved documents to a user question. \\n\n",
    "    If the documents contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be stringent test. The goal is to filter out errorneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# Example invocation for grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_text}))\n",
    "\n",
    "question = \"StateGraph\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_text}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb571a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "generate_chain = prompt | llm | StrOutputParser()\n",
    "generation = generate_chain.invoke({\"question\": question, \"context\": doc_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c82cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyDrive\\SelfStudyCode\\AI\\Krish\\AgenticAI\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1906: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model - Hallucination grader\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in the generated answer\"\"\"\n",
    "    binary_score: str = Field(description=\"\")\n",
    "\n",
    "structured_llm_hall_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system_prompt = \"\"\"You are a grader, assessing whether an LLM generation is grounded in/supported by a set of retrieved facts.\n",
    "Give a binary score 'yes' or 'no'. 'yes' means that the answer is grounded in/supported by a set of retrieved facts.\"\"\"\n",
    "\n",
    "hall_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hall_chain = hall_prompt | structured_llm_hall_grader\n",
    "\n",
    "hall_chain.invoke({\"documents\": docs, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5c2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyDrive\\SelfStudyCode\\AI\\Krish\\AgenticAI\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1906: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model - Answer grader\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses the question\"\"\"\n",
    "    binary_score: str = Field(description=\"\")\n",
    "\n",
    "structured_llm_ans_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system_prompt = \"\"\"You are a grader, assessing whether an answer addresses / resolves the question.\n",
    "Give a binary score 'yes' or 'no'. 'yes' means the answer addresses / resolves the question.\"\"\"\n",
    "\n",
    "ans_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "ans_chain = ans_prompt | structured_llm_ans_grader\n",
    "\n",
    "ans_chain.invoke({\"question\": question, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43e902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a StateGraph and how is it used in software development?\n"
     ]
    }
   ],
   "source": [
    "# Question Rewrite\n",
    "\n",
    "# Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a question re-writer that converts an input question to a better version that is optimized by re-phrasing the question with the correct and complete sentence using web search. \\n\n",
    " Look at the input and try to reason about the underlying semantic intent/meaning.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Here is the question: {question} \\n Re-write this question to make it complete and semantically correct and improve the question.\")\n",
    "    ],\n",
    ")\n",
    "rag_rewrite_chain = re_write_prompt | llm | StrOutputParser()\n",
    "response = rag_rewrite_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c484001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tausi\\AppData\\Local\\Temp\\ipykernel_8448\\3629455642.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Sachin Tendulkar | Biography, Stats, Records, Age, Centuries ...',\n",
       "  'url': 'https://www.britannica.com/biography/Sachin-Tendulkar',\n",
       "  'content': 'Sachin Tendulkar (born April 24, 1973, Bombay [Mumbai], India) is a former Indian professional cricket player, considered by many to be one of the greatest batters of all time. He is the leading run scorer in both Test cricket and one-day internationals (ODIs) and the first cricketer to score 100 centuries (100 runs in a single innings) in international cricket. Often compared to Australian great Don Bradman, Tendulkar became known for his confident stroke play off both the front and back foot. [...] Sachin Tendulkar was India’s top run scorer in the 2011 World Cup-winning campaign. He considers winning the World Cup the crowning achievement of his career.\\n\\n### What were some of Sachin Tendulkar’s achievements in the Indian Premier League (IPL)?\\n\\nIn the IPL Sachin Tendulkar played for the Mumbai Indians, captaining the team from 2008 to 2011, and was part of the squad that won the IPL title in 2013. He was the league’s leading run scorer in 2010. [...] Sachin Tendulkar is known for being one of the greatest batters of all time. He is the leading run scorer in both Test cricket and ODIs and the first cricketer to score 100 centuries in international cricket.\\n\\n### When did Sachin Tendulkar make his debut for the Indian national team?\\n\\nSachin Tendulkar made his debut for the Indian national cricket team at age 16 in November 1989, against Pakistan in Karachi.\\n\\n### What was Sachin Tendulkar’s role in India’s 2011 World Cup campaign?',\n",
       "  'score': 0.99971753},\n",
       " {'title': 'Sachin Tendulkar - Cricket Wiki - Fandom',\n",
       "  'url': 'https://cricket.fandom.com/wiki/Sachin_Tendulkar',\n",
       "  'content': 'Sachin Tendulkar(born on 24 April,1973) is a right handed batsman who played ODIs for India and recently retired from ODI, but still plays tests, he is considered the 3rd best batsman of all time just behind Don Bradman. Tendulkar has the most successful ODI career for batsman. In 2002 The Wisden ranked him 2nd in test behind Bradman and in ODI 2nd behind Viv Richards. He has represented India in over six ICC World Cup and was named man of the series in 2003 ICC World Cup in which India lost in [...] Sachin Tendulkar was born to a brahmin family on 24 April, 1973 in Dadar, Mumbai. That is he belongs to renowned Saraswat Brahmin Family. His father, Ramesh Tendulkar, was a Marathi poet and novelist. His mother was Rajni who worked in the insurance industry. He has two half-brothers namely Nitin and Ajit and one half-sister Savita. His formative years were spent in the \"Sahitya Sahawas Cooperative Housing Society\" in Bandra (East). He did his schooling from the Sharadashram Vidyamandir High [...] Do you know that the career of world-famous cricketer Sachin Tendulkar began at the age of 13 years when he made his cricket debut in the Cricket Club of India? On 11 December, 1988 in a first-class cricket match between Mumbai and Gujarat, Sachin scored 100 not out. Starting from here, he played several cricket matches at the national as well as international level.',\n",
       "  'score': 0.9993013},\n",
       " {'title': 'Sachin Tendulkar: The God of Cricket – Full Biography - PlanetSpark',\n",
       "  'url': 'https://www.planetspark.in/elements/biography-of-sachin-tendulkar',\n",
       "  'content': \"Sachin Ramesh Tendulkar was born on April 24, 1973, in Mumbai, Maharashtra, India. He was the fourth and youngest of four children and was named after the renowned music director Sachin Dev Burman. His father, Ramesh Tendulkar, was a Marathi novelist and poet of some repute, and his mother, Rajni Tendulkar, was an insurance employee. [...] Sachin Tendulkar's biography is not just a record of cricketing milestones - it is the story of a dream turned into reality through hard work, humility, and an unbreakable spirit. From playing with a broken nose to scoring hundreds against the best in the world, Sachin has shown what it means to pursue greatness with grace. [...] Sachin Tendulkar's life story is one of talent, effort, and humility. From the little prodigy to the God of Cricket, he influenced generations with his commitment and passion. His legacy transcends records - today, he is a symbol of excellence and a genuine legend in the sporting world.\\n\\n## FAQs:\\n\\n### 1. When did Sachin Tendulkar make his international debut?\\n\\nAns. Sachin made his international debut on November 15, 1989, in a Test match against Pakistan in Karachi at the age of 16.\",\n",
       "  'score': 0.99818975},\n",
       " {'title': 'The Glorious Career Of Sachin Tendulkar - YouTube',\n",
       "  'url': 'https://www.youtube.com/watch?v=EpG7eh7pH54',\n",
       "  'content': \"Sachin Tendulkar was widely regarded as the greatest batter after Sir Don Bradman. His glorious rise in the 1990s was synonymous with India's growth story in the decade. He broke all records, set new milestones, and was the inspiration of a billion Indians throughout his cricketing career which spanned over two and a half decades. As he turns 51, we look back at some numbers that defined the glorious career of Sachin Tendulkar. Watch this video for more.\",\n",
       "  'score': 0.9943141},\n",
       " {'title': 'Biography On SACHIN TENDULKAR | PDF | Cricket - Scribd',\n",
       "  'url': 'https://www.scribd.com/document/426029396/Biography-on-SACHIN-TENDULKAR',\n",
       "  'content': 'Tendulkar was born on 24 April 1973 into a Rajapur Saraswat Brahmin family in Bombay. He made an impact in cricket from a very early age, displaying a prodigious talent. By the age of 15, he had made his first-class debut. He made his test debut against Pakistan, aged only 16. Sachin Tendulkar has played a key role in Indian test cricket ever since his debut at an early age in 1992. A child cricket prodigy who has lived up to his early promise. He is one of the most decorated players in the [...] a record that could stand for a long time. In India, Sachin Tendulkar is one of the most famous people in the country. He receives a tremendous amount of adulation and attention. He is admired not just for his cricket, but his image of being a good son of India. He has a modest approach to life, being a quiet family man, and he still visits Indian temples. He is a devotee of the Hindu Guru, Sathya Sai Baba of Puttaparthi. Tendulkar is widely seen as a great role model.\\n\\n## Share this document [...] Sachin Tendulkar is an Indian cricket player who was born in 1973 in Bombay (now Mumbai). He showed his prodigious talent for cricket from a very young age, making his debut in international…\\n\\n## Uploaded by\\n\\nBig Shaq\\n\\nYou are on page 1/ 2\\n\\nBiography on SACHIN TENDULKAR',\n",
       "  'score': 0.98959166}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web search tool - Tavily\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "web_search_tool.invoke({\"query\": \"Who is Sachin tendulkar?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29821ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes as we saw in corrective RAG and create a workflow\n",
    "# Invoke the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255a2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c537e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a161a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
